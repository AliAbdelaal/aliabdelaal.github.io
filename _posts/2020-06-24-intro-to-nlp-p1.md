---
title: "Introduction to NLP in Arabic - part 1"
tagline: "نظرة عامة في مجال معالجة اللغة باستخدام خوارزميات تعلم الآلة"
excerpt: "نظرة عامة في مجال معالجة اللغة باستخدام خوارزميات تعلم الآلة"
header:
  overlay_image: https://unsplash.com/photos/6jlYDFfyuCQ/download?force=true
  overlay_filter: 0.5 # same as adding an opacity of 0.5 to a black background
  caption: "Photo credit: [**Unsplash**](https://unsplash.com)"
  teaser: https://unsplash.com/photos/6jlYDFfyuCQ/download?force=true
categories:
  - Blog
tags:
  - arabic
  - natural language processing
  - machine learning
toc: true
toc_sticky: true
toc_label: "المحتويات"
classes: wide
---

<script src="https://unpkg.com/vanilla-back-to-top@7.2.1/dist/vanilla-back-to-top.min.js"></script>
<script>addBackToTop({
  diameter: 56,
  backgroundColor: 'rgb(128, 128, 128)',
  textColor: '#fff'
})</script>

## مقدمة
{: .text-right}

<div dir='rtl'>
في هذا المقال المكون من جزئين سوف نلقي نظرة عامة على مجال معالجة النصوص باستخدام خوارزميات تعلم الآلة وسوف نلقى الضوء على تطور المجال بداية من استخدام الطرق التقليدية في معالجة النصوص مثل استخدام اسلوب (bag of words) التي تعتمد على عدد الكلمات إلى استخدام الطرق الحديثة مثل (word embedding) و كذلك استخدام تقنيات التعلم العميق (deep learning) في تحسين اداء الانظمة الحديثة التي نستخدمها حاليا في معظم المنتجات مثل انظمة الترجمة الآلية، تصحيح النصوص و البحث و غيره من التطبيقات. 
</div>

## قبل ان تكمل
{: .text-right}

<div dir='rtl'>
في هذا المقال سوف نستعرض بعض الأمثلة باستخدام لغة البايثون (Python) لذا انصحك عزيزي ان لم يكن لديك خلفية عنها ان تطلع على كورس بسيط فيها و تعود لتكمل المقال، لن نقوم بالتطرق لمواضيع متقدمة للغاية في اللغة هنا لذا لابأس ان لم تقم باستخدام اللغة منذ فترة، نحتاج فقط الاساسيات هنا.
</div>

## تطبيقات معالجة اللغة في حياتنا اليومية
{: .text-right}

<div dir='rtl'>
يمكنك ان ترى تطبيقات معالجة اللغة بداية من هاتفك المحمول حيث يمكن للوحة المفاتيح ان تقترح عليك الكلمة القادمة بناءا على اسلوبك في الكتابة و كذلك بناءا على طبيعة اللغة و يتم هذا باستخدام عدة اساليب اشهرها استخدام نموذج للغة (language model)، ايضا يمكنك ان ترى جودة محركات البحث الحالية في فهم ما تريده عن طريق النص الذي تقوم بتزويد المحرك به فيمكن للمحرك ان يفهم المعنى الكامن في النص الذي قمت بإدخاله واستدعاء نتائج بحث تماثل المعنى الذي قمت بطلبه و من الآليات التي توفر لك مثل هكذا قدرات استخدام ال (word embedding) و سوف نتعرض لها في مقالنا هنا.
</div>

<div dir='rtl'>
ليس فقط على مستوى الافراد يمكننا ان نرى هذه التطبيقات ولكن ايضا على مستوى الشركات، فعلى سبيل المثال بعض الشركات تقوم بعمل تحليل للنصوص على وسائل التواصل لتوفير معلومات عن السوق وعن ما يقوله عملائك عنك على وسائل التواصل، ويوجد شركات قامت بقطع شوط كبير في هذا المجال في وطننا العربي مثل شركة <a href="https://www.crowdanalyzer.com/">crowdanalyzer</a>.
</div>

<div dir='rtl'>
ايضا تقوم بعض الشركات بتطبيق معالجة اللغة في بناءا (chat-bots) تقوم بالتواصل معك من خلال خدمة الدردشة (chat) بدلا من التحدث مع احد ممثلي خدمة العملاء، بل ان شركة جوجل قامت بعرض خدمة جديدة تستبدل ممثل خدمة العملاء كليا اذ انك تتحث مع ال (bot) بالصوت وليس فقط من خلال النص كما هو الحال مع ال (chat-bots) التقليدية.
</div>

{% include video id="D5VN56jQMWM" provider="youtube" %}

<div dir='rtl'>
هذه فقط بعض الأمثلة البسيطة ولكن هناك العديد من التطبيقات في حياتنا يمكنك الإطلاع على بعضها من خلال <a href='https://towardsdatascience.com/natural-language-processing-nlp-top-10-applications-to-know-b2c80bd428cb'>هذا</a> الرابط
</div>>

## بعض اشهر التطبيقات
{: .text-right}

<div dir='rtl'>
في البداية دعنا نستعرض بعض التطبيقات التي نقوم باستخدام معالجة النصوص بها.
</div>

### تحديد نوع النص (text classification)
{: .text-right}

<div dir='rtl'>
اشهر التطبيقات هي تحديد نوع النص من بين انواع محددة (text classification) مثل ان نقوم بتحديد ما إذا كان النص بصيغة المدح ام الذم، على سبيل المثال عند عرض مراجعات منتج معين نريد معرفة ما اذا كانت المراجعة ايجابية ام سلبية و هذا التطبيق تحديدا يسمى (sentiment analysis) و هو تحليل للمشاعر، يمكن ايضا ان تتسع الاختيارات فتشمل انواع اخرى مختلفة مثل ان نقوم بتحديد ما اذا كان الخطاب به عنف ام لا او ان نقوم بتحديد الفئة التي ينتمي اليها النص من بين مجموعة فئات كأن يكون ترفيهي او علمي او رياضي على سبيل المثال.
</div>
<div dir='rtl'>
فكما ترى استخدامات تحديد النص كثيرة للغاية و منتشرة بشكل كبير جدا و غالبا ما تجد المصادر التعليمية تهتم بها في بداية تعلمك لمجال معالجة اللغة لاهميتها وكذلك سهولة فهمها وتطبيقها.
</div>

![text-classification](https://1.bp.blogspot.com/-zozGrHwAv9A/WNp86wRiPXI/AAAAAAAAC2M/KUsRp9NEKv8QWhdq2YIXNkkkv02IetiUwCLcB/s1600/sentiment.png){: .align-center}
<center><a href="https://blog.vicz.in/2017/03/what-is-sentiment-analysis.html">المصدر</a></center>

### الترجمة الآلية (Neural Machine Translation - NMT)
{: .text-right}

<div dir='rtl'>
احد اشهر التطبيقات التي نستخدمها هي الترجمة الآلية (Neural Machine Translation - NMT) و هي تشرح نفسها إلى حدا كبير، هي ان تقوم الآلة من تلقاء نفسها بترجمة النص بشكل اوتوماتيكي، كما ترى في خدمة <a hre='https://translate.google.com/'>ترجمة جوجل</a> على سبيل المثال.
</div>

![nmt-google]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/nmt-google.png){: .align-center}
<center><a href="www.google.com">المصدر</a></center>

### استخراج اجابة السؤال (Question Answering)
{: .text-right}

<div dir='rtl'>
من اشهر التطبيقات ايضا تطبيق الإجابة على الاسئلة واستخراج الإجابة من النص (Question Answering) وفي هنا يكون هدف البرنامج هو استخراج إجابة سؤال معطى من المستخدم من خلال قطعة نصية كما يقوم محرك البحث بإستخراج الإجابة عن سؤالك الذي قمت بكتابته في محرك البحث !
</div>

![qa-nile]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/qa-nile.png){: .align-center}
<center><a href="www.google.com">المصدر</a></center>

### استخراج اسماء الكيانات (Named Entity Recognition - NER)
{: .text-right}

<div dir='rtl'>
ايضا من التطبيقات التي يجب ذكرها هو استخراج اسماء الكيانات (Named Entity Recognition - NER) و هنا نقوم باستخراج من النص الاسماء التي تدل على مؤسسات مثلا، عملات او حتى اسماء لاشخاص.
</div>

![ner-mawdoo3]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/ner-mawdoo3.png){: .align-center}
<center><a href="https://ai.mawdoo3.com/mixed">المصدر</a></center>

<div dir='rtl'>
هذه فقط بعض التطبيقات و يوجد تطبيقات اخرى مهمة مثل تلخيص النصوص (text summarization) و استخراج الكلمات التي تشير لنفس الكيان (Coreference Resolution) وايضا تحويل النصوص إلى صوت (text to speech - TTS) و العديد من التطبيقات الأخرى، يمكنك الإطلاع عليهم وعلى احدث ما توصل إليه العلم في هذه المجالات من خلال <a href='http://nlpprogress.com/'>هذا الرابط</a>.
</div>

## كيف تعمل هذه التطبيقات
{: .text-right}

<div dir='rtl'>
معالجة النصوص تتم بأكثر من طريقة في الحقيقة، بعض الطرق يكون بسيط للغاية ولكنه ذكي إلى حد كبير إذ يعتمد بشكل اساسي على قواعد مسبقة لدى البرنامج ويقوم بتنفيذها، تطبيق بسيط للغاية لهكذا تطبيق يمكن ان يكون في الشات بوت مثلا لرد التحية.
</div>

```python
import random
replies = [ 'hi my name is bot !',
            'morning how can i help ?',
            'What can i help you with sir ?']

user_input = input("please enter a message")
if user_input in ['hello', 'welcome', 'hi', 'aloha']:
  print(random.choice(replies))
```

<div dir='rtl'>
هنا يمكنك ان ترى بوضوح ان في حالة ادخال المستخدم نص مختلف عن الذي كنت تتوقعه فلن يفهم البرنامج ما قام المستخدم بقوله، ولهذا تعتمد طريقة ال (Rule Based) على ان يقوم كاتبها بتغطية تقريبا كل الحالات التي يمكن ان يقولها المستخدم، وقد اعتادت هذه الطريقة ان تستخد في السابق و لازالت تستخدم حتى الآن منفردة في بعض التطبيقات المحدودة و ايضا تستخدم بجانب استخدام تعلم الآلة لتغطية بعض الحالات التي لدينا معرفة مسبقة عنها.
</div>

<div dir='rtl'>
و يمكننا معالجة النص باستخدام طريقة التعلم من خلال البيانات، وهذه الطريقة هي الاكثر استخداما حاليا وتشهد تطور كبير بالتبعية لتطور تطبيقات تعلم الآلة (machine learning) و تطبيقات التعلم العميق (deep learning)، وهذه الطريقة هي التي سنقوم بتغطيتها بشئ من التفاصيل في هذا المقال.
</div>

<div dir='rtl'>
ما سبق من التطبيقات تعتمد على تمثيل النصوص بشكل ما يسمح بمعالجتها بشكل سريع و ذو كفاءة عالية، و تختلف طريقة تمثيل النصوص من تطبيق لآخر لذا في هذا الجزء من المقال سوف نقوم بالتركيز على تمثيل النص بالنسبة لتطبيق تحديد النص (text classification) لسهولته كما ذكرنا من قبل.
</div>

<div dir='rtl'>
ربما تتذكر من <a href='https://aliabdelaal.github.io/blog/intro-to-ml/#%D8%A7%D9%84%D9%86%D8%B2%D9%88%D9%84-%D8%A7%D9%84%D8%AA%D8%AF%D8%B1%D9%8A%D8%AC%D9%8A-gradient-descent'>مقالنا السابق</a> عن تعلم الآلة كيفية عمل النزول التدريجي (gradient descent) وكيف انه يقوم بتحديد اوزان لمجموعة متغيرات ثم نقوم بتقدير هذه الاوزان عن طريق حساب مدى الخطأ، يمكنك الرجوع إلى المقال لاسترجاع هذه المعلومة.
</div>

![model-formulation]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-ml/formula-2.png){: .align-center}
<center><a href="https://aliabdelaal.github.io/blog/intro-to-ml/#%D8%A7%D9%84%D9%86%D8%B2%D9%88%D9%84-%D8%A7%D9%84%D8%AA%D8%AF%D8%B1%D9%8A%D8%AC%D9%8A-gradient-descent">قراءة المقال</a></center>

<div dir='rtl'>
اذا يجب ان نقوم بإستخراج خصائص لهذا النص تقوم بتوصيفه وتمييزه عن باقي النصوص، ابسط انواع الخصائص التي يمكننا استخراجها من النص هي الكلمات، فدعنا نرى كيف نستخرج الكلمات كخصائص للنص.
</div>

## تمثيل النص من خلال عدد الكلمات (bag of words - bow)
{: .text-right}

### استخراج مكونات النص (Tokenization)
{: .text-right}

<div dir='rtl'>
 يجب التركيز على نقطة مهمة قبل البدأ في استخراج الخصائص من النص و هي تعريف الكلمة، اذ ان تعريف الكلمة مؤخرا اختلف من طريقة لأخرى، لنكون دقيقين اكثر ما هو تعريف الوحدة النصية (token) في النص الذي نعمل عليه، اذا كانت كلمة فيمكن ان تكون كلمة (يحبه) و (احبه) و (تحبه) كلمات مختلفة تمام عن بعضهم البعض، بينما بعض الطرق تقوم بتفكيك الكلمة و جعلها (ي + حب + ه) و (ا + حب + ه) يمكنك ان ترى ما يحدث هنا، نحن نقوم بتفصيل النص و استخراج مكوناته، هذه العملية مهمة اذ انها الاساس لما هو قادم لان كل وحدة (token) ستعتبر خاصية من خصائص النص، اذا فمثالنا السابق عن الحب بدلا من تصبح (يحبه) صفة من صفات النص ستصبح (ي) و (حب) و (ه) من خصائص النص و هكذا عند استخدام اشكال مختلفة من الفعل ستدري ان الأصل حب.
</div>

<div dir='rtl'>
لاحظ ان هذه الطريقة لا تقوم باستخراج اساس الكلمة و هي لاتدري ان حب بالضرورة كلمة صحيحة هي تعمل على اساس معين و خطوات معينة يمكنك الإطلاع عليه من خلال <a href='https://mlexplained.com/2019/11/06/a-deep-dive-into-the-wonderful-world-of-preprocessing-in-nlp/'>هذا الرابط</a>.
</div>

### بناء قاعدة الكلمات (vocabulary)
{: .text-right}

<div dir='rtl'>
سوف نعتمد في البداية على طريقة التقسيم على اساس المسافات اي ان الوحدة النصية ستصبح الكلمة التي تأتي بعدها مسافة، لنستطيع تمثيل النصوص باستخدام هذه الخصائص يجب علينا في البداية ان نقوم بتحديد الصفات التي سوف نقوم بتوصيف النص على اساسها، في هذه الحالة الصفات هي الكلمات اذا لنقم بتحديد كل الكلمات التي يمكننا ان نصف النص بناءا عليها و سوف نسميها قاعدة الكلمات (Vocabulary) او القاموس الخاص بالنموذج الخاص بنا.
</div>

<div dir='rtl'>
دعنا نرى مثال باستخدام مكتبة sklearn في لغة البايثون.
</div>

```python
from sklearn.feature_extraction.text import CountVectorizer

text =[
    "مرحبا",
    "اهلا يا صديقي",
    "كيف حالك يا صديقي",
    "مرحبا صديقي"
]

vectorizer = CountVectorizer()
vectorizer.fit(text)
```

![count-vectorizer]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/count-vectorizer.png){: .align-center}

<div dir='rtl'>
كما ترى هنا قاعدة الكلمات الخاصة بنا هي الكلمات المميزة في جميع النصوص و يتم توصيف النص من خلال عدد الكلمات التي تظهر به، فعلى سبيل المثال يمكن ان نرى هنا ان النص "مرحبا يا صديقي" يتم تمثيله بالارقام
</div>

```python
[0, 0, 1, 0, 1, 1]
```

<div dir='rtl'>
تعد هذه الطريقة الابسط في تمثيل النصوص و يمكن استخدامها كممثل للنص كما نرى و في المثال القادم نقوم بعمل نموذج بسيط باستخدام sklearn مرة اخرى يقوم بتحديد المشاعر في النص (sentiment analysis) على بعض التغريدات <a href='https://www.kaggle.com/mksaad/arabic-sentiment-twitter-corpus'>من موقع تويتر</a>
</div>

<div dir='rtl'>
تتوفر البيانات على شكل ملفيين احدهم يحتوى على التغريدات الإيجابية و الاخر على السلبية لذا سنقوم بتجميعهم و استخراج الخصائص منهم.
</div>

```python
# read the data
pos_reviews = pd.read_csv("train_Arabic_tweets_positive_20190413.tsv", sep='\t', header=None)
neg_reviews = pd.read_csv("train_Arabic_tweets_negative_20190413.tsv", sep='\t', header=None)
dataset = pd.concat([pos_reviews, neg_reviews])
dataset.columns = ['sentiment', 'text']
dataset.head()
```

![data-head]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/sentiment-head.png){: .align-center}

```python
dataset['sentiment'].value_counts()

>>> pos    22761
>>> neg    22514
>>> Name: sentiment, dtype: int64
```

<div dir='rtl'>
هنا نرى ان البيانات متوازنة بشكل كبير اذ ان عدد التغريدات الإيجابية قريب جدا من السلبية، والان سوف نقوم بتقسيم البيانات إلى جزء للتعلم و جزء للاختبار (train test split)
</div>

```python
from sklearn.model_selection import train_test_split

x_train, x_test, y_train, y_test = train_test_split(dataset['text'], dataset['sentiment'], test_size=.2)
x_train.shape, x_test.shape
>>> ((36220,), (9055,))
```

<div dir='rtl'>
والان يمكننا ان نقوم بتكوين قاعدة الكلمات و نرى جزء منها.
</div>

```python
vectorizer = CountVectorizer(max_features=1500)
vectorizer.fit(x_train)
```

<div dir='rtl'>
هنا قمنا بتحديد عدد اقصى للخصائص التي يمكن تجميعها للحفاظ على الذاكرة و كذلك لتبسيط المثال ولكن في المتوسط تطبيقات اللغة تحتوى على كلمات اكثر بكثير.
</div>

![ar-sentiment]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/ar-sentiment.png){: .align-center}

<div dir='rtl'>
يجب ان نقوم بتحويل النص إلى ارقام كما قمنا من قبل لنستطيع استخدام نموذج تعلم آلة (machine learning) على البيانات.
</div>

```python
x_train_v = vectorizer.transform(x_train)
x_test_v = vectorizer.transform(x_test)
```

<div dir='rtl'>
الآن يمكننا ان نقوم بتدريب نموذج بسيط مثل (Logistic Regression) و هو نموذج يستخدم في التحديد والاختيار (Classification).
</div>

```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(x_train_v, y_train)
```

<div dir='rtl'>
والان يمكننا قياس جودة النموذج عن طريق مدى دقته على البيانات التي لم يراها من قبل
</div>

<div dir='rtl' class='notice--info'>
(لاحظ ان هناك طرق اخرى لقياس مدى جودة النموذج مثل ال recall او التغطية و كذلك قياس ال f1-score وهي طريقة تجمع بين ال precision (الدقة في حالة الإختيار/ classification) و ال recall/التغطية )
</div>

```python
clf.score(x_test_v, y_test)
>>> 0.7168415240198786
```

### عيوب هذه الطريقة 
{: .text-right}

<div dir='rtl'>
يمكنك ان ترى ان قاعدة الكلمات تحتوي على كلمات كثيرة مثل (في، إلا، على، وهكذا) و هذه الكلمات تسمى كلمات وقف (stopwords) فهي لاتضيف كثيرا للمعنى وانما تستخدم لترتيب و تشكيل اللجملة فهذه الكلمات تأخذ مساحة كبيرة من الكلمات ولكنها ليست ذات اهمية كبيرة.
</div>

<div dir='rtl'>
ايضا يمكننا ان نرى ان عدد الكلمات يمكن ان يزيد بشكل كبير جدا لان الوحدة الاساسية (token) بالاساس تستخرج على اساس المسافة و كما وضحنا سابقا فان هناك طرق افضل في استخراج الكلمات (tokenization) هذه الطرق تساهم بشكل كبير في تقليل المساحة المستخدمة.
</div>

<div dir='rtl'>
ايضا من المشاكل التي يمكنك ملاحظتها هي ان الترتيب لا يعتد به على الإطلاق، اذ ان الجملتين الاتيتين لهما تقريبا نفس التمثيل.
</div>

```python
reviews = [ "انا احب هذا المنتج كثيرا لا يوجد لدي اي شكوى",
            "انا لا احب هذا المنتج على الإطلاق لدى مليون شكوى"]
```

![bow-order-issue]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/bow-order-issue.png){: .align-center}

<div dir='rtl'>
فعلى الرغم من اختلافهم كليا الا ان هذه الطريقة لا تعتد بالترتيب لانه لايوجد ما يخبر البرنامج ان كلمة (احب) قد سبقها (لا)، لذا يجب ان نوجد حل لهذه المشكلة ايضا.
</div>

<div dir='rtl'>
من عيوب هذه الطريقة ايضا انها لا تعطي اي مؤشر للكلمات المتشابهة، بمعنى ان كلمة (عشق) و كلمة (حب) لا يوجد اي دلالة على تشابهما بينما ان امكنا تحصيل معلومة كهذه فقد تفيدنا جدا.
</div>

<div dir='rtl'>
ايضا ماذا إن ادخل المستخدم كلمة جديدة لم يراها النموذج اثناء التدريب، حينها سيتجاهلها النموذج بالكلية لانها لا تعتبر من الخصائص/القاموس الخاص بالنموذج.
</div>

<div dir='rtl'>
يمكنك ان ترى ان هناك العديد من المشاكل ولكن لاتقلق هناك حلول لهذه المشاكل كما سنرى فيما يلي.
</div>

## تمثيل النص من خلال معدلات التكرار و الندرة (TF-IDF)
{: .text-right}

![tf-idf](http://3.bp.blogspot.com/-u928a3xbrsw/UukmRVX_JzI/AAAAAAAAAKE/wIhuNmdQb7E/s1600/td-idf-graphic.png){: .align-center}
<center><a href="http://filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html">المصدر</a></center>

<div dir='rtl'>
هناك طريقة اخرى و هي تعديل بسيط على الطريقة السابقة لحل مشكلة الكلمات التي ليس لها اهمية كبيرة وهي عن طريق حساب معدل تكرارها في النص المستخدم و كذلك كل النصوص المتاحة، اذ ان الكلمة المستخدمة بكثرة في جميع النصوص فهي ليست مهمة بالضرورة مثل الكلمات الاقل ظهورا.
</div>

<div dir='rtl'>
تقوم الطريقة بالاساس بتعيين قيمة لكل كلمة تعبر عن اهميتها و كلما زادت اهمية الكلمة كلما زادت القيمة.
</div>

$$ W_x = tf_x \times \log(\frac{N}{df_x}) $$

<div dir='rtl'>
تقوم المعادلة بالأساس عن طريق حساب معدل تكرار الكلمة في النص الحالي (term frequency - tf) و كذلك حساب عدد النصوص التي ظهرت بها هذه الكلمة (document frequency - df) دعنا نستعرض مثال لكلمة مستخدمة كثيرا مثل حرف الجر (في) سوف نجد انها تقريبا سوف تظهر في كل النصوص وبهذا سوف تكون قيمة ال (df) كبيرة و قريبة جدا من (N) و هو عدد النصوص المتاحة اذا ستكون محصلة العملية log(N/df) قريبة من 0 لان كلما زادت قيمة df كلما نقصت قيمة الكسر و بالتبعية قيمة اللوغاريتم على عكس لو نقصت قيمة ال df كلما زادت قيمة الكسر و كذلك تزداد قيمة اللوغاريتم، الذي يرمز له غالبا بالقيمة (inverse document frequency - idf).
</div>

<div dir='rtl'>
هنا يمكن ان نرى مثال على استخدام tfidf على جزء من البيانات
</div>

![tfidf-vectorizer]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/tfidf-vectorizer.png){: .align-center}

<div dir='rtl'>
كما ترى فإن لكل كلمة قيمة ناتجة من حساب معدل التكرار وليس مجرد عددها كما كان في السابق.
</div>

<div dir='rtl'>
يمكننا تغير سطر واحد فقط في البرنامج السابق لاستخدام معدل التكرار (tf-idf) بدلا من العدد فقط (count vectorizer).
</div>

```python
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer()
vectorizer.fit(x_train)
```

<div dir='rtl'>
سوف نلاحظ زيادة طفيفة في جودة توقعاتنا ولكن سوف تظل مشاكل اخرى قائمة مثل ان (لا احب) يتم قراءتها على انها (لا) و (احب) و في هذا الإطار نرى ان كلمة احب بالتأكيد تدل على رأي إيجابي لذلك غالبا سوف يتوقع نموذجنا ان النص ايجابي، وسوف نقوم باستعراض طريقة يمكنها حل هذه المشكلة بنسبة جيدة.
</div>

## تمثيل النص باستخدام مجموعة كلمات (N-gram)
{: .text-right}

![ngram](https://vitalflux.com/wp-content/uploads/2018/02/Ngram-language-model-explained-with-examples.png){: .align-center}
<center><a href="https://vitalflux.com/n-gram-language-models-explained-examples/">المصدر</a></center>

<div dir='rtl'>
لمعالجة مشكلة مثل (لا احب) وايضا الكلمات المركبة بشكل عام مثل (أبو بكر) وايضا (حسبي الله ونعم الوكيل) على سبيل المثال نحتاج لان ننظر ليس فقط لكلمة واحدة وانما ننظر إلى عدة كلمات متتالية مرة واحدة، وهذه هي طريقة ال (ngrams) التي تعتمد على وجود نافذة بحجم معين (مثلا كلمتين) ونختار كل كلمتين متتاليتين ليصبحوا وحدة نصية (token)، اذا كانت النافذة ذات حجم 3 يصبح مسماهم (trigrams) و هكذا.
</div>

```python
text =[
    "مرحبا",
    "اهلا يا صديقي",
    "كيف حالك يا صديقي",
    "مرحبا صديقي"
]

# bigram example
vectorizer = CountVectorizer(ngram_range=(1, 2))
vectorizer.fit(text)
```

![bigrams]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/bigrams.png){: .align-center}

```python
text =[
    "مرحبا",
    "اهلا يا صديقي",
    "كيف حالك يا صديقي",
    "مرحبا صديقي"
]

# trigram example
vectorizer = CountVectorizer(ngram_range=(1, 3))
vectorizer.fit(text)
```

![trigrams]({{ site.url }}{{ site.baseurl }}/assets/images/intro-to-nlp-p1/trigrams.png){: .align-center}

<div dir='rtl'>
استخدام مجموعات الكلمات (ngram) سوف يحسن اداء النموذج الخاص بك لانه يوفر للنموذج نافذة اكبر يستطيع النظر من خلالها للنص ولكنه يزيد من عدد الخصائص بشكل كبير كما ترى و ايضا مازالت هناك بعض المشاكل التي لم نقم بحلها بعد مثل الترتيب وكذلك مدى تشابه الكلمات ذات المعاني المتقاربة.
</div>

## تمثيل النص باستخدام متجهات المعاني (word embeddings)
{: .text-right}

<div dir='rtl'>
حتى الآن كان تمثيلنا للنص مقتصر على شكل مجموعة ارقام تمثل النص ككل، ولكن الآن نريد عمل تمثيل للكلمة الواحدة في شكل متجه (vector) والذي سوف يكون مفيد في استخدامات كثيرة خاصة في مجالات التعلم العميق (deep learning) فدعنا نرى بعض اشهر الطرق في تمثيل النص على شكل متجه.
</div>

### الترميز التقليدي للكلمة الواحدة (one hot encoding)
{: .text-right}

<div dir='rtl'>
في هذه الطريقة سوف نقوم بتحديد رقم لكل كلمة من الكلمات المميزة التي لدينا ثم نقوم بتمثيل كل كلمة على شكل متجه طوله عدد الكلمات المميزة لدينا و قيمته صفر في جميع الاماكن ماعدا إحداثي الكلمة، يمكنك ان ترى بشكل اوضح في المثال الآتي.
</div>

```python
from collections import defaultdict

text = "مرحبا هلا يا صديقي كيف حالك يا صديقي مرحبا صديقي"

text2idx = defaultdict(lambda: 0)
for i, word in enumerate(set(text.split()), 1):
    text2idx[word] = i

text2idx['مرحبا']
>>> 5
```

<div dir='rtl'>
هنا قمنا ببناء المتغير <code>text2idx</code> حيث يحتوي على كل كلمة والرقم المقابل لها و استخدمنا هنا <code>defaultdict</code> حتى نحصل على قيمة 0 عندما نستخدمه مع كلمات جديدة، الآن دعنا نستخدم ما قمنا بعمله لتكوين المتجه المعبر عن الكلمة.
</div>

```python
import numpy as np
word = 'مرحبا'

word_vector = np.zeros((len(text2idx.keys())))
word_vector[text2idx[word]] = 1

word_vector
>>> array([0., 0., 0., 0., 0., 1., 0.])
```

<div dir='rtl'>
كما ترى هنا قمنا بعمل متجه فارغ بحجم مجموع الكلمات التي لدينا و من ثم قمنا بتغير قيمة احداثي هذه الكلمة فقط ليصبح قيمته 1 و اذا استخدمناه مع كلمة جديدة سوف نحصل على تمثيل مثل ما يلي.
</div>

```python
word = 'مصر'

word_vector = np.zeros((len(text2idx.keys())))
word_vector[text2idx[word]] = 1

word_vector
>>> array([1., 0., 0., 0., 0., 0., 0.])
```

<div dir='rtl'>
الكلمات التي لم نراها من قبل والتي لم تكن متواجدة في البيانات التي تمرنا عليها سوف تحصل على متجه يحتوي على 1 في الخانة الاولى التي خصصناها للكلمات الخارجة عن القاموس الخاص بنا (Out of Vocab).
</div>

<div dir='rtl'>
تمثيل النصوص بهذه الطريقة مهم للغاية حيث انه يتم استخدامه في الطرق الحديثة كما سنرى لاحقا. ولكن كما ترى فهو لا يحتوي على اي معلومة عن الكلمة ماعدا رقم الكلمة في قاموس كلماتنا و هذه ليست معلومة يمكننا من خلالها استخراج معاني مفيدة عن التشابه بين الكلمات و معانيها.
</div>

### متجهات الكلمات ذات المعنى (word2vec)
{: .text-right}

<div dir='rtl'>
تخيل ان تقوم بعمل عملية حسابية على الكلمات كالمثال الآتي.
</div>

القاهرة - مصر + فرنسا ~= باريس
{: .text-center}

<div dir='rtl'>
في هذه المعادلة نقوم باستخراج باريس بناءا على العلاقة بين مصر و القاهرة اذ ما تقوله المعادلة هو العلاقة بين القاهرة و مصر كالعلاقة بين فرنسا و ماذا ؟ بالطبع يمكنك ان تتخيل ان الإجابة باريس عاصمة فرنسا، ولكن كيف يمكننا تطبيق هذه المعادلة بالفعل اذ يقوم البرنامج بالإجابة عوضا عنا بباريس؟
</div>

<div dir='rtl'>
في <a href='https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf'>ورقتهم العلمية</a> توماس ميكولوف وزملائه قامو بإقتراح نموذج صياغة للكلمات على شكل متجه بحيث يحتوي المتجه على معلومات عن الكلمة، والكلمات التي تظهر مع نفس المحيط من الكلمات سوف تحصل على متجهات متشابهة، لن نتطرق لتفاصيل عمل النموذج في الوقت الحالي لكن يمكنك الإطلاع عليه من خلال <a href='http://jalammar.github.io/illustrated-word2vec/'> هذا المقال</a> الرائع لكن دعنا نرى كيفية استخدام هذا النموذج و كيف يمكنه ان يفيدنا في عملية معالجة النصوص من خلال مكتبة <a href='https://spacy.io/'>spaCy</a>.
</div>

```bash
$pip install spacy
$python -m spacy download en_core_web_md
```

<div dir='rtl'>
الآن قمنا بتحميل المكتبة والملفات التي نحتاجها لتوليد متجهات الكلمات، لنرى كيف يمكننا استخدامها.
</div>

```python
import spacy
nlp = spacy.load("en_core_web_md")

cat = nlp("cat")
cat.vector[:10]
>>> array([-0.15067 , -0.024468, -0.23368 , -0.23378 , -0.18382 ,  0.32711 ,
>>>       -0.22084 , -0.28777 ,  0.12759 ,  1.1656  ], dtype=float32)
```

<div dir='rtl'>
حصلنا الآن على المتجه الخاص بكلمة قطة، المتجه يحتوي على 300 رقم ولكن استعرضنا فقط اول 10 منهم، الآن دعنا نحصل على متجهيين اخريين.
</div>

```python
dog = nlp("dog")
car = nlp("car")
```

<div dir='rtl'>
لاحظ الآن كيف يمكننا مقارنة الكلمات.
</div>

```python
cat.similarity(car)
>>> 0.3190752856973872

cat.similarity(dog)
>>> 0.8016854705531046
```

<div dir='rtl'>
كما ترى على الرغم من تشابه كلمة (cat) و (car) من حيث الأحرف، إلا ان التشابه بينهم ضعيف جدا مقارنة بالتشابه بين (cat) و (dog) هذا لان المعنى اقرب بالطبع، فكما ترى ان متجهات الكلمات تتضمن المعاني بداخلها و بالطبع المعاني المتضمنة داخل المتجهات تأتي بعد تدريب معين لن يسعنى تغطيته في الوقت الحالي لكن يمكنك الإطلاع على الورقة البحثية التي اشرنا إليها.
</div>

<div dir='rtl' class='notice--info'>
تتم حساب نسبة التشابه بين المتجهات على اساس جيب الزاوية (cosine) بينهم، يمكنك الإطلاع على طريقة حسابها من <a href='https://deepai.org/machine-learning-glossary-and-terms/cosine-similarity'>هنا</a>
</div>


<div dir='rtl' class='notice--info'>
لاحظ ان مكتبة <code>spaCy</code> تستخدم نموذج مختلف قليلا عن الذي اشرنا إليه في الورقة البحثية لكنه يقوم بنفس الوظيفة بشكل افضل قليلا، يمكنك الإطلاع عليه من خلال الورقة البحثية الخاصة به من <a href='https://nlp.stanford.edu/pubs/glove.pdf'>هذا الرابط</a>
</div>

### استخدام المتجهات في تطبيق تحديد العاطفة في النص
{: .text-right}

<div dir='rtl'>
لنقوم باستخدام المتجهات مع نماذجنا يجب علينا ان نعيد تمثيل النص على هيئة متجهات، ولكن نحن نحصل على متجه لكل كلمة فكيف نحصل على متجه للنص الكامل ؟
</div>

<div dir='rtl'>
احد اشهر الطرق هي ان نقوم باستخدام مجموع المتجهات الخاصة بكل كلمة لتصبح المتجه الممثل للنص بكامله، هناك طرق اخرى سوف نتحدث عناه لاحقا ولكن الآن سوف نستخدم هذه الطريقة.
</div>

```python
text = "this is a simple text"
vector = np.zeros((300,))

for word in nlp(text):
    vector += word.vector

vector[:10]
>>> array([-0.46234499,  0.81979895, -0.83246968,  1.26062   ,  0.389594  ,
>>>        -0.428568  ,  0.24449199, -1.57974999,  0.15832401,  9.83159995])
```

<div dir='rtl'>
يمكنك ان تلاحظ هنا ان كل كلمة سوف يتم تمثيلها بمتجه يعبر عنها، ولان المتجهات الموجودة بالفعل في النموذج كثيرة وقد تكون اكثر من الكلمات التي تتدرب عليها في البرنامج الخاص بك، اذا قام المستخدم بإدخال كلمة جديدة لم يراها النموذج الخاص بك اثناء التدريب ولكن لها متجه معرف بالفعل فان النموذج الخاص بك سوف يحصل على متجه شبيه بالمتجهات التي حصل عليها اثناء تدريبه لان متجه هذه الكلمة لن يكون جديد كليا، دعنا نستعرض مثال على هذه الحالة تحديا.
</div>

<div dir='rtl'>
دعنا نقول ان اثناء التدريب نوذجك تعرض لكلمة يحب ولكنه لم يتعرض لكلمة يعشق، وعند استخدام النوذج الخاص بك ادخل المستخدم كلمة يعشق، في الحالة العادية سوف يقوم النموذج بتجاهلها ولكن في هذه الحالة (لو كان لها متجه معرف في نموذج المتجهات الذي هو مختلف عن نموذجك الخاص) حين إذ سوف تحصل منها على متجه يشبه كثيرا المتجه الخاص بكلمة (يحب) وهكذا يمكن للنموذج الخاص بك ان يفهم انها ليست كلمة جديدة كليا وانما كملة شبيهة بكلمة تعرض لها مسبقا.
</div>

### بعض الملاحظات على تمثيل النص بشكل متجهات الكلمات (word2vec/glove)
{: .text-right}

<div dir='rtl'>
لاحظ هنا ان جودة تلك المتجهات تتوقف على جودة النموذج الذي استخدم في تكوينهم، اذ انه اذا كان ضعيفا ولم يتم توفير بيانات كافيه له فلن تحصل على متجهات فعالة وجيدة و انما متجهات غير معبرة لان في اغلب الاحيان لن يرى الكلمة اكثر من مرة في اكثر من موضع حتى يفهم المحتوى الذي تظهر به ليجد شبيهاتها من الكلمات.
</div>

<div dir='rtl'>
لاحظ ايضا كيف ان المتجه الخاص بالكلمة لا يختلف بإختلاف مكان استخدامها، على سبيل المثال (صليت المغرب في المغرب) كما تلاحظ هنا ان (المغرب) الاولى تعني صلاة المغرب اما الثانية فتعني البلد، في حالة استخدامنا لنموذج مبنى بطريقة (word2vec) او طريقة (GloVe) -وهما الطريقتان المستخدمان في المتجهات التي قمنا بعرضها- سوف تحصل على نفس المتجه وهذا بالتأكيد ليس الحل الأمثل.
</div>

<div dir='rtl'>
ايضا جمع المتجهات مكننا من تحصيل المعلومات الكامنة في النص بإكمله ولكن بالتأكيد لايغني عن حاجتنا لان نأخذ في اعتبارنا ترتيب النص، لان حتى بعد ان نقوم بجمع المتجهات، لن تختلف النتيجة بإختلاف ترتيب النص فمازلنا في حاجة إلى حل تلك المشكلة.
</div>

## الاستنتاج
{: .text-right}

<div dir='rtl'>
حتى الآن قمنا بتغطية العديد من الطرق لتمثيل النص وكيفية استخدامهم بلغة البايثون لعمل تطبيق بسيط للغاية، واستعرضنا اهم العقبات والمشاكل التي تواجه هذه الطرق و هكذا كيف ان هناك طرق اخرى لحل هذه المشاكل وهذا نهاية النصف الاول من هذه المقدمة المختصرة البسيطة عن معالجة الآلة باستخدام خوارزميات تعليم الآلة.
</div>

<div dir='rtl'>
في الجزء القادم سوف نتحدث عن نماذج اخرى افضل في معالجة النصوص و كيفية معالجتها للمشاكل التي تعرضنا لها حتى الآن وايضا سنتحدث عن النماذج المستخدمة حاليا في التقنيات الحديثة التي نراها في تطبيقاتنا بشكل يومي.
</div>

<div dir='rtl' class='notice--success'>
في هذا المقال حاولت تبسيط بعض المصطلحات للغتنا العربية من اجل تسهيل عملية الشرح ولتبسيط المعلومة، في حالة اي خطأ املائي او اقتراح افضل للترجمة فأنا ارحب جدا بذلك يمكنك التعليق على المقال او مراسلتي لتعديل و تحسين المحتوى، ووفقنا الله وإياكم لما يحب ويرضى.
</div>

## مصادر
{: .text-right}

<div dir='rtl'>
<ul>
<li><a href='https://www.coursera.org/learn/language-processing'>كورس معالجة اللغة من جامعة HSE</a></li>
<li><a href='https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf'>الورقة البحثية الخاصة بنموذج word2vec</a></li>
<li><a href='http://jalammar.github.io/illustrated-word2vec/'>شرح توضيحي رائع لطريقة عمل word2vec</a></li>
<li><a href='https://nlp.stanford.edu/pubs/glove.pdf'>الورقة البحثية الخاصة بنموذج glove</a></li>
</ul>
</div>
